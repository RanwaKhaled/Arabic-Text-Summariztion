{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fine tuning T5 base arabic to perform Arabic abstractive summarization","metadata":{"id":"UGXVfafp01l1"}},{"cell_type":"code","source":"%pip install transformers[torch] evaluate datasets rouge-score nltk\n%pip install accelerate -U","metadata":{"execution":{"iopub.status.busy":"2024-09-18T05:01:27.496902Z","iopub.execute_input":"2024-09-18T05:01:27.497252Z","iopub.status.idle":"2024-09-18T05:01:57.232004Z","shell.execute_reply.started":"2024-09-18T05:01:27.497218Z","shell.execute_reply":"2024-09-18T05:01:57.230758Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Collecting evaluate\n  Using cached evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.21.0)\nCollecting rouge-score\n  Using cached rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.2.4)\nRequirement already satisfied: transformers[torch] in /opt/conda/lib/python3.10/site-packages (4.44.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.24.6)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.4.4)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (4.66.4)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.33.0)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2.4.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.2.2)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.4.0)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.16.0)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers[torch]) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (2024.7.4)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->transformers[torch]) (1.13.2)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->transformers[torch]) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->transformers[torch]) (3.1.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->transformers[torch]) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->transformers[torch]) (1.3.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: rouge-score\n  Building wheel for rouge-score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=8d885b082d6850fdaf0b2d3bcd8949ec48fec104849ccc07ee729eb4518f7272\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge-score\nInstalling collected packages: rouge-score, evaluate\nSuccessfully installed evaluate-0.4.3 rouge-score-0.1.2\nNote: you may need to restart the kernel to use updated packages.\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.33.0)\nCollecting accelerate\n  Downloading accelerate-0.34.2-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: numpy<3.0.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.2)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.4.0)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.24.6)\nRequirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.6.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.13.2)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.7.4)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nDownloading accelerate-0.34.2-py3-none-any.whl (324 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.4/324.4 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: accelerate\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.33.0\n    Uninstalling accelerate-0.33.0:\n      Successfully uninstalled accelerate-0.33.0\nSuccessfully installed accelerate-0.34.2\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2024-09-18T05:02:08.839842Z","iopub.execute_input":"2024-09-18T05:02:08.840524Z","iopub.status.idle":"2024-09-18T05:02:09.879087Z","shell.execute_reply.started":"2024-09-18T05:02:08.840475Z","shell.execute_reply":"2024-09-18T05:02:09.877944Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Wed Sep 18 05:02:09 2024       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla P100-PCIE-16GB           Off |   00000000:00:04.0 Off |                    0 |\n| N/A   36C    P0             28W /  250W |       0MiB /  16384MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"code","source":"%pip install accelerate","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%pip install arabert","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zPSQrwA3u6IA","outputId":"6aff57ad-35d1-4069-f954-e1a0bf1bfab1","execution":{"iopub.status.busy":"2024-09-18T05:02:12.070684Z","iopub.execute_input":"2024-09-18T05:02:12.071472Z","iopub.status.idle":"2024-09-18T05:02:28.022082Z","shell.execute_reply.started":"2024-09-18T05:02:12.071431Z","shell.execute_reply":"2024-09-18T05:02:28.020880Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Collecting arabert\n  Downloading arabert-1.0.1-py3-none-any.whl.metadata (16 kB)\nRequirement already satisfied: PyArabic in /opt/conda/lib/python3.10/site-packages (from arabert) (0.6.15)\nCollecting farasapy (from arabert)\n  Downloading farasapy-0.0.14-py3-none-any.whl.metadata (8.9 kB)\nCollecting emoji==1.4.2 (from arabert)\n  Downloading emoji-1.4.2.tar.gz (184 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m185.0/185.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from farasapy->arabert) (2.32.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from farasapy->arabert) (4.66.4)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from PyArabic->arabert) (1.16.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->farasapy->arabert) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->farasapy->arabert) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->farasapy->arabert) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->farasapy->arabert) (2024.7.4)\nDownloading arabert-1.0.1-py3-none-any.whl (179 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading farasapy-0.0.14-py3-none-any.whl (11 kB)\nBuilding wheels for collected packages: emoji\n  Building wheel for emoji (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for emoji: filename=emoji-1.4.2-py3-none-any.whl size=186459 sha256=33d0cde14c88e29ffe18fefbf026779cd01b94e02cda4fab0ef89d5149ad88be\n  Stored in directory: /root/.cache/pip/wheels/10/f0/fd/4813b1177405693e8da9cdea839f0fb64fde161380e058c827\nSuccessfully built emoji\nInstalling collected packages: emoji, farasapy, arabert\n  Attempting uninstall: emoji\n    Found existing installation: emoji 2.12.1\n    Uninstalling emoji-2.12.1:\n      Successfully uninstalled emoji-2.12.1\nSuccessfully installed arabert-1.0.1 emoji-1.4.2 farasapy-0.0.14\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"%pip install datasets","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lGgRBq2dJzBR","outputId":"f404e787-ad19-4f45-e147-a0d2cd3ded7b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%pip install evaluate","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_1_8NNQ94Wxh","outputId":"1394a386-2e5d-443f-9e4c-956694db4a6e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%pip install rouge_score","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xz_S2PiH4v4t","outputId":"9ce8e990-59d5-4d8b-b6fe-f3ce1510e1d8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","metadata":{"execution":{"iopub.status.busy":"2024-09-18T05:02:28.024256Z","iopub.execute_input":"2024-09-18T05:02:28.024614Z","iopub.status.idle":"2024-09-18T05:02:28.029591Z","shell.execute_reply.started":"2024-09-18T05:02:28.024579Z","shell.execute_reply":"2024-09-18T05:02:28.028531Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Loading the dataset\nLoading the **AGS-Corpus** with arabic articles and their summaries from *HuggingFace*","metadata":{"id":"-knL-t_wcwrl"}},{"cell_type":"code","source":"import torch\nfrom datasets import load_dataset\n\nds = load_dataset(\"FahdSeddik/AGS-Corpus\", split=[\"train\"])","metadata":{"id":"KtHO2FABzrgX","execution":{"iopub.status.busy":"2024-09-18T05:02:28.030944Z","iopub.execute_input":"2024-09-18T05:02:28.031675Z","iopub.status.idle":"2024-09-18T05:02:32.720700Z","shell.execute_reply.started":"2024-09-18T05:02:28.031632Z","shell.execute_reply":"2024-09-18T05:02:32.719758Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/4.68k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43b35af515d248a6800b4f1241f142f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/190M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8470ce71a3ce4d81a2d79cfcc5ec6d70"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/141467 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ab34ad590094d4e90a717304473006e"}},"metadata":{}}]},{"cell_type":"markdown","source":"Getting the dataset","metadata":{"id":"wKg58REsc82B"}},{"cell_type":"code","source":"train_ds = ds[0]\ntrain_ds","metadata":{"id":"UPC7tK5PHvmr","execution":{"iopub.status.busy":"2024-09-18T05:02:32.723249Z","iopub.execute_input":"2024-09-18T05:02:32.724090Z","iopub.status.idle":"2024-09-18T05:02:32.731196Z","shell.execute_reply.started":"2024-09-18T05:02:32.724053Z","shell.execute_reply":"2024-09-18T05:02:32.730146Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['id', 'text', 'summary'],\n    num_rows: 141467\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"Now that we've loaded the dataset from HuggingFace we can look at an example","metadata":{"id":"tNZFZXB5coz_"}},{"cell_type":"code","source":"example = train_ds[0]\narticle = example['text']\nsummary = example['summary']\nprint(\"Article\\n\", article)\nprint(\"Summary\\n\", summary)","metadata":{"id":"mdbr2khBLiwm","execution":{"iopub.status.busy":"2024-09-18T05:02:32.732383Z","iopub.execute_input":"2024-09-18T05:02:32.732765Z","iopub.status.idle":"2024-09-18T05:02:33.133268Z","shell.execute_reply.started":"2024-09-18T05:02:32.732724Z","shell.execute_reply":"2024-09-18T05:02:33.132268Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Article\n يمكن تصور السلوكيات المُهَدِدة باعتبارها ثمرة لعدم القدرة على التأقلم مع الدافع الطبيعي التنافسي المتعلق بعلاقات الهيمنة المتبادلة التي تلاحظ عامة بين الحيوانات. بدلاً من ذلك، قد ينتج الترهيب في مجتمع من نوع يكون أفراده اجتماعيين، فالبشر بشكل عام يترددون في الدخول في مواجهة أو تهديد عنيف.وهو سلوك مثله مثل جميع السمات السلوكية يظهر بشكل أزيد أو أقل في كل فرد مع مرور الزمن، ولكنه قد يكون «سلوك تعويضي» ذو أهمية كبيرة بالنسبة للبعض مقارنة بالآخرين. فإن المنظرين السلوكيين كثيراً ما يرون أن السلوكيات المُهَدِدة هي نتيجة لتعرض القائمين بها للتهديد من قبل الآخرين، بما في ذلك الآباء، ورموز السلطة، والرفاق والأشقاء. «استخدام القوة مبرر عندما يعتقد الشخص بشكل منطقي أنها ضرورية للدفاع عن النفس أو الآخرين تجاه الاستخدام الفوري لقوة غير شرعية».و قد يتم استخدام الترهيب بوعي أو بغير وعي، ونسبة من الأشخاص الذين يستخدمونه بوعي ربما يفعلون ذلك نتيجة أفكار مستوعبة بأنانية عن تخصيصه لغرض، أو لفائدة أو للتمكين الذاتي. الترهيب المتصل بالتحامل والتمييز يمكن أن يشمل السلوك «الذي يزعج، يهدد، يرهب، وينذر، أو يضع الشخص في حالة خوف على سلامته... بسبب اعتقاد أو تصور بشأن عرقه أو لونه أو أصله القومي أو نسبه أو جنسه أو دينه أو ممارسته لشعائر دينية أو سنه أو إعاقته أو توجهه الجنسي، بغض النظر عن ما إذا كان ذلك الاعتقاد أو التصور صحيحاً».قد يتجلى الترهيب بطرق مختلفة مثل الاحتكاك البدني، أو تجهم الطلعة، أو التلاعب بالمشاعر، أو الإساءة اللفظية، أو جعل شخصاً يشعر بأنه أقل منك، أو الإحراج المتعمد و/أو الاعتداء الجسدي الصريح. «السلوك قد يشمل، ولكنه ليس محصوراً في، الألقاب، التعليقات والافتراءات المهينة والمقترحات البذيئة والاعتداء والإعاقة وعرقلة أو منع الحركة، واللمس الجارح أو أي تداخل جسدي في الحركة أو العمل العادي، والإهانات المرئية، مثل الملصقات أو الرسوم المهينة».ليس هناك تعريف قانوني في القانون الإنجليزي بشأن ما يشكله سلوك «الترهيب» Intimidation، ولذا فالأمر متروك للمحاكم لاتخاذ قرارها بشأن كل حالة على حدة. ومع ذلك، إذا هدد شخص بالعنف تجاه شخص آخر، فان ذلك قد يشكل جريمة جنائية.\nSummary\n السلوكيات المهددة يمكن أن تكون نتيجة عدم القدرة على التأقلم مع العلاقات الهيمنة التنافسية المتبادلة بين الحيوانات. قد ينتج الترهيب في مجتمع من النوع الاجتماعي. الترهيب يتم استخدامه بوعي أو بغير وعي، ونسبة الأشخاص الذين يستخدمونه بوعي ربما يفعلون ذلك بسبب أفكار أنانية. الترهيب يمكن أن يشمل الاحتكاك البدني، أو التلاعب بالمشاعر، أو الإساءة اللفظية، أو الإحراج المتعمد و/أو الاعتداء الجسدي. قد يشكل تهديد العنف جريمة جنائية.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Encoding the dataset\nEncoding the dataset using the tokenizer and add the \"Task prefix\" as the T5 model needs the summarization prefix\n  \n### Preprocessing using the `ArabertPreprocessor`\nThe ArabertPreprocessor is designed to normalize Arabic text for better performance when using Arabic NLP models. Specifically, it does things like:\n\n* **Removing diacritics** (which are optional in Arabic but can add noise for models not trained with them).\n* **Normalizing different forms of Arabic letters** (like \"أ\", \"إ\", \"آ\" being reduced to \"ا\").\n* **Handling Arabic-specific punctuation and word forms** that may affect tokenization   ","metadata":{"id":"hU19qltndSpZ"}},{"cell_type":"code","source":"# import the tokenizer\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n# import preprocessor\nfrom arabert.preprocess import ArabertPreprocessor\n\n# initialize the tokenizer from the model\ntokenizer = AutoTokenizer.from_pretrained(\"malmarjeh/t5-arabic-text-summarization\")\n# intialize preprocessor\npreprocessor = ArabertPreprocessor(model_name = \"\")\n\nprefix = 'تلخيص: '\nmax_input_length = 512\nmax_output_length = 200\n\ndef preprocess_examples(examples):\n  # preprocess using the arabertpreprocessor\n  articles = [preprocessor.preprocess(article) for article in examples['text']]\n  summaries = [preprocessor.preprocess(summary) for summary in examples['summary']]\n\n  # add the prefix to the input\n  inputs = [prefix + article for article in articles]\n  # tokenize inputs\n  tokenized_inputs =  tokenizer(inputs, max_length = max_input_length, padding= \"max_length\", truncation = True)\n  # encode the summaries\n  labels = tokenizer(summaries, max_length=max_output_length, padding=\"max_length\", truncation=True).input_ids\n\n  # important: we need to replace the index of the padding tokens by -100\n  # such that they are not taken into account by the CrossEntropyLoss\n  labels_with_ignore_index = []\n  for labels_example in labels:\n    labels_example = [label if label != 0 else -100 for label in labels_example]\n    labels_with_ignore_index.append(labels_example)\n\n  tokenized_inputs[\"labels\"] = labels_with_ignore_index\n\n  return tokenized_inputs","metadata":{"id":"44e-5DwvdOdz","execution":{"iopub.status.busy":"2024-09-18T05:02:33.134797Z","iopub.execute_input":"2024-09-18T05:02:33.135185Z","iopub.status.idle":"2024-09-18T05:02:36.353973Z","shell.execute_reply.started":"2024-09-18T05:02:33.135140Z","shell.execute_reply":"2024-09-18T05:02:36.353164Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/335 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e8ef030e9b44c20923d39f43066ece5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/679 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58b083b06ebf4bcaa612f5630773a3e9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/2.44M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13391072e08546258af968182b2c0848"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"508226da1c694f1c82b0f23f9904663b"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"# encode the dataset\nencoded_dataset =  train_ds.map(preprocess_examples, batched=True, remove_columns=train_ds.column_names)","metadata":{"id":"JvGa8reI2bNT","execution":{"iopub.status.busy":"2024-09-18T05:02:36.355104Z","iopub.execute_input":"2024-09-18T05:02:36.355617Z","iopub.status.idle":"2024-09-18T05:12:39.790173Z","shell.execute_reply.started":"2024-09-18T05:02:36.355583Z","shell.execute_reply":"2024-09-18T05:12:39.789157Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/141467 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98232f6361754d329dd7395c1937aa19"}},"metadata":{}}]},{"cell_type":"markdown","source":"verifying an example by decoding back to text","metadata":{"id":"YY52KW5BNTeM"}},{"cell_type":"code","source":"labels = encoded_dataset[0]['labels']\nprint(labels)\ntokenizer.decode([x for x in labels if x != -100])","metadata":{"id":"a-5a6eMANSq_","execution":{"iopub.status.busy":"2024-09-18T05:12:39.791429Z","iopub.execute_input":"2024-09-18T05:12:39.791777Z","iopub.status.idle":"2024-09-18T05:12:51.047106Z","shell.execute_reply.started":"2024-09-18T05:12:39.791740Z","shell.execute_reply":"2024-09-18T05:12:51.046102Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"[58335, 83393, 345, 24, 268, 1280, 493, 4727, 15, 79492, 46, 1090, 39826, 30219, 30471, 87, 9622, 3, 4, 140, 15303, 60, 47587, 8, 4519, 6, 4279, 2387, 3, 4, 60, 47587, 520, 30365, 71963, 82, 3080, 8778, 3, 5, 41199, 2627, 194, 26477, 13, 71963, 1586, 23075, 133, 408, 9257, 24, 5462, 3, 4, 60, 47587, 345, 24, 8230, 64421, 51593, 3, 5, 82, 30327, 63222, 3, 5, 82, 19926, 88261, 28, 3, 5, 82, 60, 44750, 67818, 14, 50, 82, 8264, 62642, 3, 4, 140, 4454, 9648, 2445, 5038, 33398, 3, 4, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"'السلوكيات المهددة يمكن أن تكون نتيجة عدم القدرة على التأقلم مع العلاقات الهيمنة التنافسية المتبادلة بين الحيوانات. قد ينتج الترهيب في مجتمع من النوع الاجتماعي. الترهيب يتم استخدامه بوعي أو بغير وعي ، ونسبة الأشخاص الذين يستخدمونه بوعي ربما يفعلون ذلك بسبب أفكار أنانية. الترهيب يمكن أن يشمل الاحتكاك البدني ، أو التلاعب بالمشاعر ، أو الإساءة اللفظية ، أو الإحراج المتعمد و - أو الاعتداء الجسدي. قد يشكل تهديد العنف جريمة جنائية.</s>'"},"metadata":{}}]},{"cell_type":"markdown","source":"Now we set the format to PyTorch","metadata":{"id":"ONbir_FaNnpf"}},{"cell_type":"code","source":"encoded_dataset.set_format(type = 'torch')","metadata":{"id":"Esvmno9JNruS","execution":{"iopub.status.busy":"2024-09-18T05:12:51.048536Z","iopub.execute_input":"2024-09-18T05:12:51.049620Z","iopub.status.idle":"2024-09-18T05:12:51.054389Z","shell.execute_reply.started":"2024-09-18T05:12:51.049572Z","shell.execute_reply":"2024-09-18T05:12:51.053368Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"split the dataset to train/test sets","metadata":{"id":"KjOv91ESrfYF"}},{"cell_type":"code","source":"from torch.utils.data import DataLoader, random_split\nfrom torchvision import datasets, transforms\n\ndef split_and_create_loaders(dataset, train_batch_size = 8, val_batch_size=16, test_batch_size=16):\n    # define train and test size\n    train_size = int(0.8*len(dataset))\n    val_size = int(0.1*len(dataset))\n    test_size = len(dataset)- (val_size + train_size)\n\n    # split using the random_split function\n    train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n    # create a dataloader for each set\n    train_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size = val_batch_size, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size = test_batch_size, shuffle= True)\n\n    return train_loader, val_loader, test_loader","metadata":{"id":"rlIO4KVQroKl","execution":{"iopub.status.busy":"2024-09-18T05:12:51.057592Z","iopub.execute_input":"2024-09-18T05:12:51.057943Z","iopub.status.idle":"2024-09-18T05:12:52.414821Z","shell.execute_reply.started":"2024-09-18T05:12:51.057911Z","shell.execute_reply":"2024-09-18T05:12:52.413866Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"train_loader, val_dataset, test_loader = split_and_create_loaders(encoded_dataset)","metadata":{"id":"T5G804xU0Rh1","execution":{"iopub.status.busy":"2024-09-18T05:12:52.416047Z","iopub.execute_input":"2024-09-18T05:12:52.416773Z","iopub.status.idle":"2024-09-18T05:12:52.450564Z","shell.execute_reply.started":"2024-09-18T05:12:52.416727Z","shell.execute_reply":"2024-09-18T05:12:52.449791Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"## Fine-tuning the model","metadata":{"id":"iN-9Ks9dRXpT"}},{"cell_type":"markdown","source":"`Seq2SeqTrainer` and `Seq2SeqTrainingArguments` inherit from the `Trainer` and `TrainingArgument` classes and they’re adapted for training models for sequence-to-sequence tasks such as summarization or translation. Which is why we will use those in the fine-tuning process","metadata":{"id":"kk-jlS_Ng2F-"}},{"cell_type":"code","source":"from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\nfrom transformers import DataCollatorForSeq2Seq, AdamW, AutoModelForSeq2SeqLM\nimport torch\n\ntorch.cuda.empty_cache()\n# load the model\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"malmarjeh/t5-arabic-text-summarization\")\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)  # Move the model to the appropriate device\n\n# define training arguments object\ntraining_arguments = Seq2SeqTrainingArguments(\n    output_dir = \"./results\",  # output directory\n    eval_strategy = \"epoch\",  # evaluating every epoch\n    learning_rate = 2e-5,\n    per_device_train_batch_size = 8,  # batch size for training\n    per_device_eval_batch_size = 16,  # batch size for evaluation\n    weight_decay = 0.01,\n    save_total_limit = 2,  # only keep the last 2 models\n    num_train_epochs = 1,  # nbr of  training epochs\n    predict_with_generate = True,  # enable text generation for text evaluation\n    fp16=True,\n    report_to=\"none\",  # Disables Weights & Biases\n)\n\n# initialize data collator\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=model)","metadata":{"id":"6Jm9DOvtmgvr","execution":{"iopub.status.busy":"2024-09-18T05:12:57.765640Z","iopub.execute_input":"2024-09-18T05:12:57.766487Z","iopub.status.idle":"2024-09-18T05:13:06.478107Z","shell.execute_reply.started":"2024-09-18T05:12:57.766449Z","shell.execute_reply":"2024-09-18T05:13:06.477272Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.13G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45185e2f9c7c4739acbfbc64227ed473"}},"metadata":{}}]},{"cell_type":"markdown","source":"### This run's hyperparameters:\n`epochs` 1  \n`learning rate` 0.00002  \n`max input length` 512  \n`max output length` 200 \nIn the next run we want to see how it will handle summarization of longer sequences into short paragraphs","metadata":{}},{"cell_type":"code","source":"# initialize trainer\ntrainer = Seq2SeqTrainer(\n    model,\n    training_arguments,\n    train_dataset = train_loader.dataset,\n    eval_dataset = test_loader.dataset,\n    data_collator = data_collator,\n    tokenizer = tokenizer,\n)\n\ntrainer.train()","metadata":{"id":"dHH1lm9S3Bzl","execution":{"iopub.status.busy":"2024-09-18T05:13:09.686727Z","iopub.execute_input":"2024-09-18T05:13:09.688367Z","iopub.status.idle":"2024-09-18T09:26:28.692327Z","shell.execute_reply.started":"2024-09-18T05:13:09.688314Z","shell.execute_reply":"2024-09-18T09:26:28.691412Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n/opt/conda/lib/python3.10/site-packages/transformers/data/data_collator.py:656: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /usr/local/src/pytorch/torch/csrc/utils/tensor_new.cpp:278.)\n  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='14147' max='14147' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [14147/14147 4:13:15, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>2.659500</td>\n      <td>2.204502</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=14147, training_loss=2.8351976272368367, metrics={'train_runtime': 15198.1278, 'train_samples_per_second': 7.447, 'train_steps_per_second': 0.931, 'total_flos': 6.891759134834688e+16, 'train_loss': 2.8351976272368367, 'epoch': 1.0})"},"metadata":{}}]},{"cell_type":"code","source":"eval_results = trainer.evaluate()","metadata":{"execution":{"iopub.status.busy":"2024-09-18T09:32:36.331854Z","iopub.execute_input":"2024-09-18T09:32:36.332864Z","iopub.status.idle":"2024-09-18T09:42:20.492128Z","shell.execute_reply.started":"2024-09-18T09:32:36.332803Z","shell.execute_reply":"2024-09-18T09:42:20.491249Z"},"trusted":true},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='885' max='885' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [885/885 09:43]\n    </div>\n    "},"metadata":{}}]},{"cell_type":"code","source":"eval_results","metadata":{"execution":{"iopub.status.busy":"2024-09-18T09:45:03.285331Z","iopub.execute_input":"2024-09-18T09:45:03.285746Z","iopub.status.idle":"2024-09-18T09:45:03.291812Z","shell.execute_reply.started":"2024-09-18T09:45:03.285706Z","shell.execute_reply":"2024-09-18T09:45:03.290943Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"{'eval_loss': 2.2045016288757324,\n 'eval_runtime': 584.1544,\n 'eval_samples_per_second': 24.22,\n 'eval_steps_per_second': 1.515,\n 'epoch': 1.0}"},"metadata":{}}]},{"cell_type":"code","source":"# save the fine-tuned model\ntrainer.save_model(\"/kaggle/outputs/fine_tuned_model\")\ntokenizer.save_pretrained(\"/kaggle/outputs/fine_tuned_model\")","metadata":{"execution":{"iopub.status.busy":"2024-09-18T09:50:00.034476Z","iopub.execute_input":"2024-09-18T09:50:00.035341Z","iopub.status.idle":"2024-09-18T09:50:03.707229Z","shell.execute_reply.started":"2024-09-18T09:50:00.035302Z","shell.execute_reply":"2024-09-18T09:50:03.706253Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"('/kaggle/outputs/fine_tuned_model/tokenizer_config.json',\n '/kaggle/outputs/fine_tuned_model/special_tokens_map.json',\n '/kaggle/outputs/fine_tuned_model/spiece.model',\n '/kaggle/outputs/fine_tuned_model/added_tokens.json',\n '/kaggle/outputs/fine_tuned_model/tokenizer.json')"},"metadata":{}}]},{"cell_type":"code","source":"# save the fine-tuned model\ntrainer.save_model(\"./model\")\ntokenizer.save_pretrained(\"./model\")","metadata":{"execution":{"iopub.status.busy":"2024-09-18T10:09:14.836151Z","iopub.execute_input":"2024-09-18T10:09:14.836794Z","iopub.status.idle":"2024-09-18T10:09:17.201385Z","shell.execute_reply.started":"2024-09-18T10:09:14.836750Z","shell.execute_reply":"2024-09-18T10:09:17.200386Z"},"trusted":true},"execution_count":38,"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"('./model/tokenizer_config.json',\n './model/special_tokens_map.json',\n './model/spiece.model',\n './model/added_tokens.json',\n './model/tokenizer.json')"},"metadata":{}}]},{"cell_type":"code","source":"import shutil\nimport zipfile\nimport os\nfrom IPython.display import FileLink\n\n# Path to the folder you want to zip\nfolder_path = '/kaggle/working/fine_tuned_model'\n# Path where you want to save the zip file\nzip_file_path = '/kaggle/working/fine_tuned_model.zip'\n\n# Create a zip file of the folder\nshutil.make_archive(zip_file_path.replace('.zip', ''), 'zip', folder_path)\n\nprint(f\"Model saved and zipped at: {zip_file_path}\")\n\nFileLink('/kaggle/working/fine_tuned_model.zip')","metadata":{"execution":{"iopub.status.busy":"2024-09-18T10:04:59.217321Z","iopub.execute_input":"2024-09-18T10:04:59.217757Z","iopub.status.idle":"2024-09-18T10:06:05.190019Z","shell.execute_reply.started":"2024-09-18T10:04:59.217712Z","shell.execute_reply":"2024-09-18T10:06:05.188876Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"Model saved and zipped at: /kaggle/working/fine_tuned_model.zip\n","output_type":"stream"},{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/fine_tuned_model.zip","text/html":"<a href='/kaggle/working/fine_tuned_model.zip' target='_blank'>/kaggle/working/fine_tuned_model.zip</a><br>"},"metadata":{}}]},{"cell_type":"code","source":"FileLink(r'fine_tuned_model.zip')","metadata":{"execution":{"iopub.status.busy":"2024-09-18T10:19:25.311677Z","iopub.execute_input":"2024-09-18T10:19:25.312561Z","iopub.status.idle":"2024-09-18T10:19:25.318854Z","shell.execute_reply.started":"2024-09-18T10:19:25.312520Z","shell.execute_reply":"2024-09-18T10:19:25.317826Z"},"trusted":true},"execution_count":45,"outputs":[{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/fine_tuned_model.zip","text/html":"<a href='fine_tuned_model.zip' target='_blank'>fine_tuned_model.zip</a><br>"},"metadata":{}}]},{"cell_type":"markdown","source":"### Testing on an external aritcle","metadata":{}},{"cell_type":"code","source":"# Load the fine-tuned model and tokenizer\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"/kaggle/outputs/fine_tuned_model\")\ntokenizer = AutoTokenizer.from_pretrained(\"/kaggle/outputs/fine_tuned_model\")\n\n# Example external text to summarize\nexternal_text = \"قدمت ملك ريان مذيعة موقع صدي البلد تغطية خاصة حول وقف خطة تخفيف الأحمال  حيث انتهت أمس المدة المقررة لوقف خطة تخفيف الأحمال والجميع أصبح يتساءل عن موعد عودة العمل بخطة تخفيف الأحمال من جديد فهل بالفعل سيتم قطع الكهرباء مرة أخرى؟ أم سيتم تجديد فترة وقف قطع الكهرباء؟ وما هي القرارات الجديدة المنتظر إعلانها بهذا الشأن؟دعونا نتابع معكم كل التفاصيل الخاصة بما سيحدث بشأن خطة تخفيف الأحمال خلال الأيام المقبلة من خلال هذه التغطية.انتهت أمس المدة التي حددها مجلس الوزراء برئاسة الدكتور مصطفى مدبولي، لوقف خطة تخفيف الأحمال، حيث كان من المقرر أن تكون مدة إيقاف قطع الكهرباء وتعليق خطة تخفيف الأحمال تكون من 21 يوليو الماضي وتستمر حتى يوم أمس الأحد 15 سبتمبر. ولذلك يترقب المصريون القرار الجديد الذي ستتخذه الحكومة حول مصير خطة تخفيف أحمال الكهرباء، واصبح البحث عن موعد عودة تخفيف أحمال الكهرباء متصدر محرك البحث جوجل خلال الساعات الماضية.ولكن حتى هذه اللحظة لم تصدر الحكومة أو وزارتا الكهرباء والطاقة المتجددة والبترول والثروة المعدنية أية بيانات رسمية، حتى الآن، تؤكد عودة تخفيف الأحمال وفي إطار جهودها لتأمين احتياجات شبكة الكهرباء وتجنب عودة تخفيف الأحمال، أبرمت وزارة البترول والثروة المعدنية عقودًا لشحنات جديدة من الغاز المسال كما أكد مصدر مسؤول في قطاع الكهرباء أنه لم يصل أي قرار إلى شركات الكهرباء بشأن العودة إلى تنفيذ خطة تخفيف الأحمال قبل تعليقها في 21 يوليو الماضي ومن جانبه، صرح حمدي عبد العزيز، المتحدث الرسمي لوزارة البترول والثروة المعدنية بأن عدد الشحنات المتعاقد عليها زاد من 21 شحنة إلى 32 شحنة، في إطار جهود ضمان استمرار تدفق الغاز إلى محطات توليد الكهرباء، وأشار إلى أن الغاز المسال المستورد يتم تحويله إلى غاز طبيعي وضخه في الشبكة القومية للكهرباء عبر منشآت متخصصة، مثل السفينة الموجودة في شرم الشيخ، التي تقوم بعملية التحويل وضخ الغاز بشكل مستمر لضمان استقرار إمدادات الكهرباء.وبالنسبة لإمكانية العودة لتخفيف أحمال الكهرباء قريبا، أكد عبد العزيز أنه من الصعب تأكيد أو نفي هذا الاحتمال حاليًا، وطمأن المصريين بأنه لا يوجد نقص في هذه الإمدادات الضرورية لتوليد الكهرباء.\"\n\n# Preprocess the text (tokenize)\ninputs = tokenizer(external_text, return_tensors=\"pt\", max_length=512, truncation=True).input_ids\n\n# Generate the summary using the model\nsummary_ids = model.generate(inputs, max_length=150, num_beams=5, length_penalty=1.2, early_stopping=True)\n\n# Decode the generated summary\nsummary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n\n# Print the summary\nprint(\"Generated Summary:\\n\", summary)","metadata":{"execution":{"iopub.status.busy":"2024-09-18T10:18:31.526118Z","iopub.execute_input":"2024-09-18T10:18:31.526500Z","iopub.status.idle":"2024-09-18T10:18:54.196548Z","shell.execute_reply.started":"2024-09-18T10:18:31.526466Z","shell.execute_reply":"2024-09-18T10:18:54.195687Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stderr","text":"You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n","output_type":"stream"},{"name":"stdout","text":"Generated Summary:\n أعلنت وزارة البترول والطاقة المتجددة والبترول والثروة المعدنية عن وقف خطة تخفيف الأحمال بعد انتهاء المدة المقررة لوقفها في 21 يوليو الماضي ، وتم توقيع عقود لشحنات جديدة من الغاز المسال ، وتم تحويل الغاز المسال إلى غاز طبيعي وضخه في الشبكة القومية للكهرباء. لم تصدر الحكومة أو وزارات الكهرباء والطاقة المتجددة والبترول والثروة المعدنية أي بيانات رسمية تؤكد عودة تخفيف الأحمال. لم يصدر أي قرار بشأن العودة إلى تنفيذ خطة تخفيف الأحمال بعد تعليقها في 21 يوليو الماضي.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Original Article:\nقدمت ملك ريان مذيعة موقع صدي البلد تغطية خاصة حول وقف خطة تخفيف الأحمال  حيث انتهت أمس المدة المقررة لوقف خطة تخفيف الأحمال والجميع أصبح يتساءل عن موعد عودة العمل بخطة تخفيف الأحمال من جديد فهل بالفعل سيتم قطع الكهرباء مرة أخرى؟ أم سيتم تجديد فترة وقف قطع الكهرباء؟ وما هي القرارات الجديدة المنتظر إعلانها بهذا الشأن؟\nدعونا نتابع معكم كل التفاصيل الخاصة بما سيحدث بشأن خطة تخفيف الأحمال خلال الأيام المقبلة من خلال هذه التغطية.\n\nانتهت أمس المدة التي حددها مجلس الوزراء برئاسة الدكتور مصطفى مدبولي، لوقف خطة تخفيف الأحمال، حيث كان من المقرر أن تكون مدة إيقاف قطع الكهرباء وتعليق خطة تخفيف الأحمال تكون من 21 يوليو الماضي وتستمر حتى يوم أمس الأحد 15 سبتمبر. \nولذلك يترقب المصريون القرار الجديد الذي ستتخذه الحكومة حول مصير خطة تخفيف أحمال الكهرباء، واصبح البحث عن موعد عودة تخفيف أحمال الكهرباء متصدر محرك البحث جوجل خلال الساعات الماضية.\n\nولكن حتى هذه اللحظة لم تصدر الحكومة أو وزارتا الكهرباء والطاقة المتجددة والبترول والثروة المعدنية أية بيانات رسمية، حتى الآن، تؤكد عودة تخفيف الأحمال.\nوفي إطار جهودها لتأمين احتياجات شبكة الكهرباء وتجنب عودة تخفيف الأحمال، أبرمت وزارة البترول والثروة المعدنية عقودًا لشحنات جديدة من الغاز المسال. \nكما أكد مصدر مسؤول في قطاع الكهرباء أنه لم يصل أي قرار إلى شركات الكهرباء بشأن العودة إلى تنفيذ خطة تخفيف الأحمال قبل تعليقها في 21 يوليو الماضي.\n\nومن جانبه، صرح حمدي عبد العزيز، المتحدث الرسمي لوزارة البترول والثروة المعدنية بأن عدد الشحنات المتعاقد عليها زاد من 21 شحنة إلى 32 شحنة، في إطار جهود ضمان استمرار تدفق الغاز إلى محطات توليد الكهرباء، وأشار إلى أن الغاز المسال المستورد يتم تحويله إلى غاز طبيعي وضخه في الشبكة القومية للكهرباء عبر منشآت متخصصة، مثل السفينة الموجودة في شرم الشيخ، التي تقوم بعملية التحويل وضخ الغاز بشكل مستمر لضمان استقرار إمدادات الكهرباء.\n\nوبالنسبة لإمكانية العودة لتخفيف أحمال الكهرباء قريبا، أكد عبد العزيز أنه من الصعب تأكيد أو نفي هذا الاحتمال حاليًا، وطمأن المصريين بأنه لا يوجد نقص في هذه الإمدادات الضرورية لتوليد الكهرباء.\n\n##### Generated Summary\n أعلنت وزارة البترول والطاقة المتجددة والبترول والثروة المعدنية عن وقف خطة تخفيف الأحمال بعد انتهاء المدة المقررة لوقفها في 21 يوليو الماضي وتستمر حتى 15 سبتمبر. تم توقيع عقود لشحنات جديدة من الغاز المسال ، ولكن لم يصل أي قرار إلى شركات الكهرباء بشأن العودة إلى تنفيذ خطة تخفيف الأحمال قبل تعليقها في 21 يوليو الماضي. يتم تحويل الغاز المسال إلى غاز طبيعي وضخه في الشبكة القومية للكهرباء. لا يوجد نقص في الإمدادات الضرورية لتوليد الكهرباء.","metadata":{}},{"cell_type":"code","source":"# Load the fine-tuned model and tokenizer\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"/kaggle/outputs/fine_tuned_model\")\ntokenizer = AutoTokenizer.from_pretrained(\"/kaggle/outputs/fine_tuned_model\")\n\n# Example external text to summarize\nexternal_text = \"لقد أصبح الذكاء الاصطناعي مصطلحًا شاملًا للتطبيقات التي تؤدي مهام مُعقدة كانت تتطلب في الماضي إدخالات بشرية مثل التواصل مع العملاء عبر الإنترنت أو ممارسة لعبة الشطرنج. يُستخدم غالبًا هذا المصطلح بالتبادل مع مجالاته الفرعية، والتي تشمل التعلم الآلي (ML) والتعلم العميق.ومع ذلك، هناك اختلافات.. على سبيل المثال، يُركز التعلم الآلي على إنشاء أنظمة تتعلم أو تحسّن من أدائها استنادًا إلى البيانات التي تستهلكها. ومن المهم أن نلاحظ أنه على الرغم من أن كل سُبل التعلم الآلي ما هي إلّا ذكاء اصطناعي، فإنه ليس كل ذكاء اصطناعي يُعد تعلمًا آليًا.للحصول على القيمة الكاملة من الذكاء الاصطناعي، تقوم العديد من الشركات باستثمارات كبيرة في فرق علوم البيانات. يجمع علم البيانات بين الإحصاءات وعلوم الكمبيوتر والمعرفة بالأعمال لاستخلاص القيمة من مصادر البيانات المختلفة. يستخدم المطورون الذكاء الاصطناعي لأداء المهام التي يتم تنفيذها يدويًا بكفاءة أكبر، والتواصل مع العملاء، وتحديد الأنماط، وحل المشكلات. للبدء في استخدام الذكاء الاصطناعي، يجب أن يكون للمطورين خلفية في الرياضيات ويشعرون بالراحة مع الخوارزميات.عند البدء باستخدام الذكاء الاصطناعي لإنشاء تطبيق، يساعد على البدء على نطاق صغير. من خلال بناء مشروع بسيط نسبيًا، مثل على سبيل المثال، ستتعلم أساسيات الذكاء الاصطناعي. يعد التعلم عن طريق الممارسة وسيلة رائعة لتحسين أي مهارة، والذكاء الاصطناعي لا يختلف عن ذلك. بمجرد الانتهاء من مشروع صغير أو أكثر بنجاح، لا توجد حدود للمكان الذي يمكن أن يأخذك فيه الذكاء الاصطناعي.\"\n# Preprocess the text (tokenize)\ninputs = tokenizer(external_text, return_tensors=\"pt\", max_length=512, truncation=True).input_ids\n\n# Generate the summary using the model\nsummary_ids = model.generate(inputs, max_length=150, num_beams=5, length_penalty=1.2, early_stopping=True)\n\n# Decode the generated summary\nsummary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n\n# Print the summary\nprint(\"Generated Summary:\\n\", summary)","metadata":{"execution":{"iopub.status.busy":"2024-09-18T16:44:45.889270Z","iopub.execute_input":"2024-09-18T16:44:45.889937Z","iopub.status.idle":"2024-09-18T16:45:02.902420Z","shell.execute_reply.started":"2024-09-18T16:44:45.889897Z","shell.execute_reply":"2024-09-18T16:45:02.901496Z"},"trusted":true},"execution_count":79,"outputs":[{"name":"stdout","text":"Generated Summary:\n الذكاء الاصطناعي مصطلح شامل للتطبيقات التي تؤدي مهام مُعقدة كانت تتطلب إدخالات بشرية. يمكن استخدام هذا المصطلح بالتبادل مع مجالاته الفرعية وتشمل التعلم الآلي والتعلم العميق والتعلم العميق. يستخدم المطورون الذكاء الاصطناعي لأداء المهام التي يتم تنفيذها يدويا بكفاءة أكبر والتواصل مع العملاء وتحديد الأنماط وحل المشكلات. يستخدم المطورون الذكاء الاصطناعي لأداء المهام التي يتم تنفيذها يدويا بكفاءة أكبر والتواصل مع العملاء وتحديد الأنماط وحل المشكلات. يجب أن يكون للمطورين خلفية في الرياضيات ويشعرون بالراحة مع الخوارزميات. التعلم عن طريق الممارسة وسيلة رائعة لتحسين أي مهارة.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Example 2: Original article\nلقد أصبح الذكاء الاصطناعي مصطلحًا شاملًا للتطبيقات التي تؤدي مهام مُعقدة كانت تتطلب في الماضي إدخالات بشرية مثل التواصل مع العملاء عبر الإنترنت أو ممارسة لعبة الشطرنج. يُستخدم غالبًا هذا المصطلح بالتبادل مع مجالاته الفرعية، والتي تشمل التعلم الآلي (ML) والتعلم العميق.\n\nومع ذلك، هناك اختلافات.. على سبيل المثال، يُركز التعلم الآلي على إنشاء أنظمة تتعلم أو تحسّن من أدائها استنادًا إلى البيانات التي تستهلكها. ومن المهم أن نلاحظ أنه على الرغم من أن كل سُبل التعلم الآلي ما هي إلّا ذكاء اصطناعي، فإنه ليس كل ذكاء اصطناعي يُعد تعلمًا آليًا.\n\nللحصول على القيمة الكاملة من الذكاء الاصطناعي، تقوم العديد من الشركات باستثمارات كبيرة في فرق علوم البيانات. يجمع علم البيانات بين الإحصاءات وعلوم الكمبيوتر والمعرفة بالأعمال لاستخلاص القيمة من مصادر البيانات المختلفة.\nيستخدم المطورون الذكاء الاصطناعي لأداء المهام التي يتم تنفيذها يدويًا بكفاءة أكبر، والتواصل مع العملاء، وتحديد الأنماط، وحل المشكلات. للبدء في استخدام الذكاء الاصطناعي، يجب أن يكون للمطورين خلفية في الرياضيات ويشعرون بالراحة مع الخوارزميات.\n\nعند البدء باستخدام الذكاء الاصطناعي لإنشاء تطبيق، يساعد على البدء على نطاق صغير. من خلال بناء مشروع بسيط نسبيًا، مثل tic-tac-toe، على سبيل المثال، ستتعلم أساسيات الذكاء الاصطناعي. يعد التعلم عن طريق الممارسة وسيلة رائعة لتحسين أي مهارة، والذكاء الاصطناعي لا يختلف عن ذلك. بمجرد الانتهاء من مشروع صغير أو أكثر بنجاح، لا توجد حدود للمكان الذي يمكن أن يأخذك فيه الذكاء الاصطناعي.\n\n#### Generated summary\n الذكاء الاصطناعي مصطلح شامل للتطبيقات التي تؤدي مهام مُعقدة كانت تتطلب إدخالات بشرية. يمكن استخدام هذا المصطلح بالتبادل مع مجالاته الفرعية وتشمل التعلم الآلي والتعلم العميق والتعلم العميق. يستخدم المطورون الذكاء الاصطناعي لأداء المهام التي يتم تنفيذها يدويا بكفاءة أكبر والتواصل مع العملاء وتحديد الأنماط وحل المشكلات. يستخدم المطورون الذكاء الاصطناعي لأداء المهام التي يتم تنفيذها يدويا بكفاءة أكبر والتواصل مع العملاء وتحديد الأنماط وحل المشكلات. يجب أن يكون للمطورين خلفية في الرياضيات ويشعرون بالراحة مع الخوارزميات. التعلم عن طريق الممارسة وسيلة رائعة لتحسين أي مهارة","metadata":{}},{"cell_type":"markdown","source":"### Calculating the rouge score","metadata":{}},{"cell_type":"markdown","source":"Method to compute the ROUGE sore to evalute the model","metadata":{"id":"DdPKFHXk0TpJ"}},{"cell_type":"code","source":"import torch\nfrom tqdm import tqdm\n\nmodel.eval()\n\n# initializing progress bar \nprogress_bar = tqdm(total=len(test_loader))\n\nall_preds = []\nall_labels = []\n\n# Loop through the test_loader to generate predictions\nfor batch in test_loader:\n    with torch.no_grad():\n        # Get the input IDs and labels from the batch\n        input_ids = batch['input_ids'].to(model.device)\n        labels = batch['labels'].to(model.device)\n        \n        # Generate summaries/predictions\n        preds = model.generate(input_ids, max_length=150, num_beams=2, length_penalty=1.2)\n\n        # Collect predictions and labels\n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n        \n    # Update progress bar\n    progress_bar.update(1)\n\n# Close the progress bar\nprogress_bar.close()","metadata":{"execution":{"iopub.status.busy":"2024-09-18T10:56:08.907844Z","iopub.execute_input":"2024-09-18T10:56:08.908775Z","iopub.status.idle":"2024-09-18T15:52:19.581783Z","shell.execute_reply.started":"2024-09-18T10:56:08.908730Z","shell.execute_reply":"2024-09-18T15:52:19.580444Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":46,"outputs":[{"name":"stderr","text":" 29%|██▉       | 257/885 [4:55:16<11:32:02, 66.12s/it]","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[46], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Generate summaries/predictions\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m150\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlength_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Collect predictions and labels\u001b[39;00m\n\u001b[1;32m     23\u001b[0m all_preds\u001b[38;5;241m.\u001b[39mextend(preds\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:2063\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2055\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2056\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2057\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   2058\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2059\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2060\u001b[0m     )\n\u001b[1;32m   2062\u001b[0m     \u001b[38;5;66;03m# 14. run beam sample\u001b[39;00m\n\u001b[0;32m-> 2063\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_beam_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2064\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2065\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeam_scorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2066\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2067\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2068\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2069\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2070\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2071\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2072\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2074\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGROUP_BEAM_SEARCH:\n\u001b[1;32m   2075\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2076\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2077\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2078\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2084\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2085\u001b[0m     )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:3205\u001b[0m, in \u001b[0;36mGenerationMixin._beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, generation_config, synced_gpus, logits_warper, **model_kwargs)\u001b[0m\n\u001b[1;32m   3202\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   3204\u001b[0m \u001b[38;5;66;03m# prepare variable output controls (note: some models won't accept all output controls)\u001b[39;00m\n\u001b[0;32m-> 3205\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_attentions\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_attentions} \u001b[38;5;28;01mif\u001b[39;00m output_attentions \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m   3206\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m   3208\u001b[0m \u001b[38;5;66;03m# if sequential is True, split the input to batches of batch_size and run sequentially\u001b[39;00m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"markdown","source":"The prediction was stopped around `30%` of the dataset so any metrics we shall calculate might be influenced by the fact that we didn't use all `13k` samples, However based on the performance of the model on out-of-sample web articles and compared to other trials of fine tuning it's performing quite well given the hardware and time limitations we're working with ","metadata":{}},{"cell_type":"code","source":"# import nltk\n# import numpy as np\n# import evaluate\n\n# metric = evaluate.load(\"rouge\")\n\n# def compute_metrics(eval_pred):\n#     predictions, labels = eval_pred\n#     decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n#     # Replace -100 in the labels as we can't decode them.\n#     labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n#     decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n#     # Rouge expects a newline after each sentence\n#     decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n#     decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n\n#     # Note that other metrics may not have a `use_aggregator` parameter\n#     # and thus will return a list, computing a metric for each sentence.\n#     result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True, use_aggregator=True, tokenizer=lambda x: x.split())\n#     # Extract a few results\n#     result = {key: value * 100 for key, value in result.items()}\n\n#     # Add mean generated length\n#     prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n#     result[\"gen_len\"] = np.mean(prediction_lens)\n\n#     return {k: round(v, 4) for k, v in result.items()}","metadata":{"id":"ptI7WPUC0Z69","execution":{"iopub.status.busy":"2024-09-18T10:19:12.614427Z","iopub.execute_input":"2024-09-18T10:19:12.614859Z","iopub.status.idle":"2024-09-18T10:19:13.886695Z","shell.execute_reply.started":"2024-09-18T10:19:12.614822Z","shell.execute_reply":"2024-09-18T10:19:13.885844Z"},"trusted":true},"execution_count":44,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5dbad8ab628f498d91812aabb1cb9cbd"}},"metadata":{}}]},{"cell_type":"code","source":"def rouge_score(eval_pred):\n    predictions, labels = eval_pred\n    \n    # Ensure predictions and labels are lists of arrays\n    flat_preds = [pred.tolist() for pred in predictions]\n    flat_labels = [label.tolist() for label in labels]\n\n    # Replace -100 in predictions and labels with the padding token\n    processed_preds = []\n    for pred in flat_preds:\n        processed_pred = np.where(np.array(pred) != -100, pred, tokenizer.pad_token_id)\n        processed_preds.append(processed_pred)\n\n    processed_labels = []\n    for label in flat_labels:\n        processed_label = np.where(np.array(label) != -100, label, tokenizer.pad_token_id)\n        processed_labels.append(processed_label)\n\n    # Decode the predictions and labels\n    decoded_preds = tokenizer.batch_decode(processed_preds, skip_special_tokens=True)\n    decoded_labels = tokenizer.batch_decode(processed_labels, skip_special_tokens=True)\n\n    # ROUGE expects a newline after each sentence\n    decoded_preds = [\"\\n\".join(pred.strip() for pred in pred.split('.')) for pred in decoded_preds]\n    decoded_labels = [\"\\n\".join(label.strip() for label in label.split('.')) for label in decoded_labels]\n\n    # Compute ROUGE score\n    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n\n    # Format the result\n    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n    return result","metadata":{"execution":{"iopub.status.busy":"2024-09-18T16:11:49.947832Z","iopub.execute_input":"2024-09-18T16:11:49.948520Z","iopub.status.idle":"2024-09-18T16:11:49.958914Z","shell.execute_reply.started":"2024-09-18T16:11:49.948469Z","shell.execute_reply":"2024-09-18T16:11:49.957922Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"# Compute ROUGE score using the compute_metrics() function\neval_preds = (all_preds, all_labels)\nmetrics = rouge_score(eval_preds)\n\nprint(\"ROUGE Scores:\", metrics)","metadata":{"execution":{"iopub.status.busy":"2024-09-18T16:11:51.646968Z","iopub.execute_input":"2024-09-18T16:11:51.647353Z","iopub.status.idle":"2024-09-18T16:11:54.871009Z","shell.execute_reply.started":"2024-09-18T16:11:51.647315Z","shell.execute_reply":"2024-09-18T16:11:54.870033Z"},"trusted":true},"execution_count":77,"outputs":[{"name":"stdout","text":"ROUGE Scores: {'rouge1': 12.213894325036048, 'rouge2': 4.340387739657083, 'rougeL': 11.971243158961158, 'rougeLsum': 12.096626129851842}\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}